---
title: "LCELä¸LangChainæµå¼å¤„ç†æŒ‡å—"
date: 2025-01-25T22:00:38+08:00
draft: false
image: 1.png
categories:
    - computer
    - AI
---

# LCELä¸LangChainæµå¼å¤„ç†æŒ‡å—

## ä»€ä¹ˆæ˜¯LCELï¼Ÿ

LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„å·¥ä½œæµç¼–æ’å·¥å…·ï¼Œèƒ½å¤Ÿä»åŸºæœ¬ç»„ä»¶æ„å»ºå¤æ‚ä»»åŠ¡é“¾æ¡(chain)ï¼Œå¹¶å…·æœ‰ä»¥ä¸‹ç”Ÿäº§çº§ç‰¹æ€§ï¼š

- ğŸš€ **ä¸€æµçš„æµå¼æ”¯æŒ**ï¼šç›´æ¥ä»LLMæµå¼ä¼ è¾“æ ‡è®°åˆ°è¾“å‡ºè§£æå™¨
- âš¡ **å¼‚æ­¥æ”¯æŒ**ï¼šåŒä¸€ä»£ç å¯åŒæ­¥/å¼‚æ­¥è¿è¡Œï¼Œé€‚åˆåŸå‹åˆ°ç”Ÿäº§
- ğŸ”€ **ä¼˜åŒ–çš„å¹¶è¡Œæ‰§è¡Œ**ï¼šè‡ªåŠ¨å¹¶è¡Œå¯å¹¶è¡Œæ­¥éª¤
- ğŸ”„ **é‡è¯•å’Œå›é€€**ï¼šé…ç½®å¯é æ€§æœºåˆ¶
- ğŸ” **ä¸­é—´ç»“æœè®¿é—®**ï¼šå®æ—¶ç›‘æ§å’Œè°ƒè¯•
- ğŸ“ **è¾“å…¥è¾“å‡ºæ¨¡å¼**ï¼šè‡ªåŠ¨ç”ŸæˆPydantic/JSONSchemaéªŒè¯

## Runnableæ¥å£æ ‡å‡†

LangChainç»„ä»¶é€šè¿‡Runnableåè®®å®ç°æ ‡å‡†åŒ–è°ƒç”¨ï¼š

### åŒæ­¥æ–¹æ³•
- `stream()`: æµå¼è¿”å›å“åº”å—
- `invoke()`: è°ƒç”¨é“¾å¤„ç†è¾“å…¥
- `batch()`: æ‰¹é‡å¤„ç†è¾“å…¥åˆ—è¡¨

### å¼‚æ­¥æ–¹æ³•
- `astream()`: å¼‚æ­¥æµå¼å“åº”
- `ainvoke()`: å¼‚æ­¥è°ƒç”¨
- `abatch()`: å¼‚æ­¥æ‰¹é‡å¤„ç†
- `astream_log()`: æµå¼ä¸­é—´æ­¥éª¤+æœ€ç»ˆç»“æœ
- `astream_events()`: æµå¼é“¾äº‹ä»¶(Beta)

### ç»„ä»¶I/Oç±»å‹
| ç»„ä»¶       | è¾“å…¥ç±»å‹               | è¾“å‡ºç±»å‹   |
| ---------- | ---------------------- | ---------- |
| æç¤º       | å­—å…¸                   | æç¤ºå€¼     |
| èŠå¤©æ¨¡å‹   | å­—ç¬¦ä¸²/æ¶ˆæ¯åˆ—è¡¨/æç¤ºå€¼ | èŠå¤©æ¶ˆæ¯   |
| LLM        | å­—ç¬¦ä¸²/æ¶ˆæ¯åˆ—è¡¨/æç¤ºå€¼ | å­—ç¬¦ä¸²     |
| è¾“å‡ºè§£æå™¨ | LLM/èŠå¤©æ¨¡å‹è¾“å‡º       | è§£æå™¨ç‰¹å®š |
| æ£€ç´¢å™¨     | å­—ç¬¦ä¸²                 | æ–‡æ¡£åˆ—è¡¨   |
| å·¥å…·       | å­—ç¬¦ä¸²/å­—å…¸            | å·¥å…·ç‰¹å®š   |

## æµå¼å¤„ç†(Stream)

### æ ¸å¿ƒæ–¹æ³•
1. **åŒæ­¥æµå¼**ï¼š`stream()`
2. **å¼‚æ­¥æµå¼**ï¼š`astream()`

### å…³é”®ç‰¹æ€§
- å®æ—¶è¿”å›æ¯ä¸ªå¤„ç†å—
- è¦æ±‚æ‰€æœ‰æ­¥éª¤æ”¯æŒæµå¼å¤„ç†
- å¤æ‚åº¦èŒƒå›´ï¼š
  - ç®€å•ï¼šLLMä»¤ç‰Œæµ
  - å¤æ‚ï¼šéƒ¨åˆ†JSONç»“æœæµ

### æœ€ä½³å®è·µ
å»ºè®®ä»LLMç»„ä»¶å¼€å§‹é€æ­¥æ„å»ºæµå¼å¤„ç†é“¾ã€‚

## äº‹ä»¶æµ(Stream Events)

### é«˜çº§æ–¹æ³•
- `astream_events()` (Beta): æµå¼ä¼ è¾“é“¾æ‰§è¡Œè¿‡ç¨‹ä¸­çš„æ‰€æœ‰äº‹ä»¶
- `astream_log()`: æµå¼ä¸­é—´æ­¥éª¤+æœ€ç»ˆè¾“å‡º

### å…¸å‹åº”ç”¨åœºæ™¯
1. å®æ—¶ç”¨æˆ·åé¦ˆ
2. å¤æ‚é“¾è°ƒè¯•
3. è¿›åº¦ç›‘æ§
4. å¢é‡ç»“æœå±•ç¤º

# ä»£ç ç¤ºä¾‹

```python
import asyncio
import logging

from langchain.chains.llm import LLMChain
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool

from langchain.chains import SimpleSequentialChain

import PromptStudy
from common.modeCommon import Model
from config.config import Config


# LangChainå·¥ä½œæµç¼–æ’

# åŒæ­¥Streamè¿è¡Œ
def sync_stream(model: Model, constr):
    for ch in model.qwen_llm_stream().stream(constr):
        print(ch.content, end='|', flush=True)


# @tool
# å¼‚æ­¥AStreamè¿è¡Œ
async def async_stream(model: Model, constr):
    async for ch in model.qwen_llm_stream().astream(constr):
        print(ch.content, end='|', flush=True)


# Chain
async def llm_chain(model: Model, prompt: PromptStudy):
    # åˆ›å»ºä¸€ä¸ªå·¥ä½œé“¾
    base_prompt = prompt.base_prompt()
    parser = StrOutputParser()
    stream_model = model.qwen_llm_stream()
    chain = base_prompt | stream_model | parser # åˆ›å»ºä¸€ä¸ªåˆ›å»ºä¸€ä¸ªå·¥ä½œé“¾ å…ˆæ‰§è¡Œbase_prompt ç„¶åæ‰§è¡Œstream_model æœ€åæ‰§è¡Œparser
    # è¿è¡Œå·¥ä½œé“¾
    async for ch in chain.astream(prompt.base_prompt_invoke("ç³»ç»Ÿè¿ç»´", "è¿ç»´é«˜çº§å·¥ç¨‹å¸ˆ", "æƒ³ç†è§£äº¤æ¢æœºåŸç†", "æ— ", "ç®€çŸ­æ–‡å­—è¾“å‡º")):
        print(ch, end='|', flush=True)
# åºåˆ—é“¾ï¼ˆSequential Chainsï¼‰
"""
    åºåˆ—é“¾æ˜¯ä¸€ç§å°†å¤šä¸ªé“¾æŒ‰ç…§é¡ºåºè¿æ¥åœ¨ä¸€èµ·çš„å·¥ä½œæµã€‚
    æ¯ä¸ªé“¾éƒ½å¯ä»¥æ¥æ”¶å‰ä¸€ä¸ªé“¾çš„è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™ä¸‹ä¸€ä¸ªé“¾ã€‚
    åºåˆ—é“¾å¯ä»¥ç”¨äºæ„å»ºå¤æ‚çš„å·¥ä½œæµï¼Œå…¶ä¸­æ¯ä¸ªé“¾éƒ½æœ‰ç‰¹å®šçš„ä»»åŠ¡å’Œé€»è¾‘ã€‚
    åºåˆ—é“¾çš„ä¸»è¦ä¼˜ç‚¹æ˜¯å¯ä»¥å°†å¤šä¸ªé“¾ç»„åˆåœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„å·¥ä½œæµã€‚
    åºåˆ—é“¾çš„ä¸»è¦ç¼ºç‚¹æ˜¯æ¯ä¸ªé“¾éƒ½éœ€è¦ç‹¬ç«‹è¿è¡Œï¼Œå› æ­¤å¯èƒ½ä¼šå¯¼è‡´æ€§èƒ½é—®é¢˜ã€‚
"""
def one_chain(model: Model, prompt: PromptStudy):
    base_prompt = prompt.base_prompt()
    parser = JsonOutputParser()
    stream_model = model.qwen_llm_stream()
    chain = base_prompt | stream_model | parser
    return chain
def two_chain(model: Model, prompt: PromptStudy):
    base_prompt = prompt.base_prompt()
    parser = StrOutputParser()
    stream_model = model.qwen_llm_stream()
    chain = base_prompt | stream_model | parser
    return chain
def llm_chain_sequential(model: Model, prompt: PromptStudy):
    chain_one = one_chain(model, prompt)
    chain_two = two_chain(model, prompt)
    overall_chain = chain_one | chain_two
    # è¿è¡Œå·¥ä½œé“¾
    # å°† prompt.base_prompt_invoke çš„è¿”å›å€¼åŒ…è£…æˆå­—å…¸
    # input_dict =prompt.base_prompt_invoke("ç³»ç»Ÿè¿ç»´", "è¿ç»´é«˜çº§å·¥ç¨‹å¸ˆ", "æƒ³ç†è§£äº¤æ¢æœºåŸç†", "æ— ", "ç®€çŸ­æ–‡å­—è¾“å‡º")
    # è¿è¡Œå·¥ä½œé“¾
    input_dict = {
        "domain": "é«˜é“",
        "profession": "è‡ªæ„¿è€…",
        "do": "å»å“ªé‡Œè´­ä¹°é«˜é“ç¥¨ï¼Œæˆ‘åº”è¯¥å»æ‰¾è°",
        "material": "æ— ",
        "question": "æŒ‰ç…§ä»¥ä¸‹jsonçš„æ ¼å¼è¾“å‡ºï¼š{"
                    "\"domain\": \"\","
                    "\"profession\": \"\","
                    "\"do\": \"\","
                    "\"material\": \"\", "
                    "\"question\": \"\"}"
    }
    for ch in overall_chain.stream(input_dict):
        print(ch, end='|', flush=True)

#è½¬æ¢é“¾ï¼ˆTransform Chainsï¼‰ï¼š å…è®¸ä½ åœ¨ é“¾ çš„ä¸­é—´æ­¥éª¤ä¸­å¯¹æ•°æ®è¿›è¡Œè½¬æ¢
def transform_chain(model: Model):
    first_prompt = ChatPromptTemplate.from_template("æè¿°ä¸€å®¶ç”Ÿäº§{äº§å“}çš„å…¬å¸æœ€å¥½çš„åå­—æ˜¯ä»€ä¹ˆï¼Ÿ")
    chain_one = model.qwen_llm_china(first_prompt)
    second_prompt = ChatPromptTemplate.from_template("ä¸ºä»¥ä¸‹å…¬å¸å†™ä¸€ä¸ª20å­—çš„æè¿°ï¼š{company_name}")
    chain_two = model.qwen_llm_china(second_prompt)
    overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)
    product = "å¤–å–å’Œç”µå•†"
    chain_output = overall_simple_chain.invoke(product)
    print(chain_output)
#æ¡ä»¶é“¾ï¼ˆConditional Chainsï¼‰ï¼š å…è®¸ä½ æ ¹æ®æŸäº›æ¡ä»¶é€‰æ‹©ä¸åŒçš„é“¾æ¥æ‰§è¡Œ
#è·¯ç”±é“¾ï¼ˆRouter Chainsï¼‰ï¼š å…è®¸ä½ æ ¹æ®æŸäº›æ¡ä»¶é€‰æ‹©ä¸åŒçš„é“¾æ¥æ‰§è¡Œ
#å¤šæ¨¡å‹é“¾ï¼ˆMulti Model Chainsï¼‰ï¼š å…è®¸ä½ åœ¨ä¸åŒçš„æ¨¡å‹ä¹‹é—´åˆ‡æ¢



if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    # åˆå§‹åŒ–æ¨¡å‹é…ç½®
    config = Config('conf/config.yml')
    # åˆå§‹åŒ–æ¨¡å‹
    model = Model(config)
    # åˆå§‹åŒ–æ¨¡æ¿
    prompt = PromptStudy.Prompt()
    # sync_stream(model, "è´µå·çœçš„ç»„æˆ")
    # asyncio.run(async_stream(model, "ä¸Šæµ·æ€ä¹ˆæ ·"))
    # asyncio.run(llm_chain(model, prompt))
    # llm_chain_sequential(model, prompt)
    transform_chain(model)
```

## ä»£ç ä»“åº“è·¯å¾„

[zcj-git520/AiLargeModel: å¤§æ¨¡å‹åº”ç”¨å¼€å‘å­¦ä¹ ](https://github.com/zcj-git520/AiLargeModel)
